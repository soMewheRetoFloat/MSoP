{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实验报告\n",
    "### 计21 展子航 2022010745\n",
    "\n",
    "## 引言（碎碎念）\n",
    "虽然中途发了四天烧，但是我的大作业总算还是按时完成了。其实本次收获最大的部分是我查询资料和快速学习新库的能力，同时也增加了不少码力（毕竟还是写了很多的），只希望大模型demo能对我好一些hh\n",
    "\n",
    "说来其实爬虫对我算很温柔的，因为我曾经有自学过前端三巨头并轻度使用过一些网页源代码查看工具，所以我上手是比较快的，再加上有科协的爬虫课程（我看完了2个小时）的辅助，起步要容易很多，当时身体有些不适也被我略过去了。\n",
    "\n",
    "到后来入门django的时候我就开始生病了（上周三上课时就不舒服，之后高烧了一天，又断断续续烧了三天，周日才算彻底退烧），其中四天基本是躺着度过的，可谓是非常痛苦，感觉自己能做完作业已经非常不错了555。\n",
    "\n",
    "到后来退烧以后就开始狂赶进度，有一种项目加急上线的美感，对python的基本工具已经信手拈来了（笑），最终赶上了周一的验收，除了搜索引擎有不少提升空间以外，别的已经尽力能做到达标了。\n",
    "\n",
    "这个故事告诉我们需要学会爱护自己的身体，不要在之前写得入魔了天天熬夜到三点，这样子后面一场病使得前面赶的进度全部水漂~\n",
    "## 环境准备\n",
    "anaconda下的base环境以及自建的mach虚拟环境，主要使用mach环境来进行开发。其中爬虫和数据分析部分更多是在vscode下完成（因为pycharm如果要看ipynb就要专业版软件）而web部分更多是在pycharm下完成（变量和文件更名方便、会自动重构整个代码的引用，非常省时间）\n",
    "\n",
    "## 作业介绍\n",
    "### 爬虫部分\n",
    "* 爬取的网站：新浪滚动新闻；\n",
    "* 所使用的方法：寻找新浪后端供给新闻链接条目的api，最终发现是https://feed.mix.sina.com.cn/api/roll，然后利用JSON解析器直接找到内部存留的所有链接，存储在一个txt文件中；\n",
    "* 爬取的url量：从4月1日到8月1日数据量在8776条："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from time import sleep as slp\n",
    "from typing import List\n",
    "import random\n",
    "import pandas as pd\n",
    "import re\n",
    "from datetime import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"User-Agent\": \n",
    "            \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/116.0.0.0 Safari/537.36 Edg/116.0.1938.54\",\n",
    "            } #伪装成浏览器访问\n",
    "\n",
    "def get_urls(urlx, start_page, dt) -> List[str]: #写一个爬其中一页的函数\n",
    "    dt_c = datetime.strptime(dt, '%Y-%m-%d')\n",
    "    dt_stamp = int(dt_c.timestamp())\n",
    "    #print(dt_stamp)\n",
    "    st_stamp = dt_stamp + 86400\n",
    "    news_list = []\n",
    "    try:\n",
    "        repo = requests.get(url = urlx, params = {\"pageid\": 153, \"lid\": 2515, \"num\": 50, \"date\": dt, \"page\": start_page, \"stime\": st_stamp, \"ctime\": st_stamp, \"etime\": dt_stamp}, headers = config)\n",
    "        repo.encoding = \"UTF-8\"\n",
    "        tex = repo.json() # 解析\n",
    "        # 然后直接找链接\n",
    "        leng = len(tex[\"result\"][\"data\"])\n",
    "        news_list += [ tex[\"result\"][\"data\"][index][\"url\"] for index in range(leng)] \n",
    "    except:\n",
    "        news_list.append[\"cannot reach\"]\n",
    "    return news_list\n",
    "\n",
    "\n",
    "\n",
    "dates = pd.date_range(start = \"2023-07-01\", periods = 30).strftime(\"%Y-%m-%d\").tolist()\n",
    "# dates这个数据是自行修改的\n",
    "dates = dates[::-1]\n",
    "for dt in dates:\n",
    "    for page in range(1, 10):\n",
    "        lst = get_urls(f\"https://feed.mix.sina.com.cn/api/roll/get\", page, dt)\n",
    "        with open(r'./news_links_lasty.txt', 'a', encoding = \"utf-8\") as f:\n",
    "            for item in lst:\n",
    "                if item != \"cannot reach\":\n",
    "                    f.write(f\"{item}\\n\")\n",
    "            f.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "之后处理新闻要素，本质上是使用BeautifulSoup4用lxml解析器解析并且处理html文本，然后将所有的图片中缺失http前缀的全部补上方便下载，之后提取所有我想要的要素，如果实在提取不到就用正则表达式匹配原文本。由于我发现新浪的新闻有三种html格式，其中第一种和第二种高度相像，第三种要更特别一些，因此先运行以下代码，将输出的无法使用的url留存："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#第二步：处理新闻要素\n",
    "from PIL import Image\n",
    "import traceback\n",
    "import multiprocessing\n",
    "config = {\"User-Agent\": \n",
    "            \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/116.0.0.0 Safari/537.36 Edg/116.0.1938.54\",\n",
    "        }\n",
    "\n",
    "def get_html(url):\n",
    "    slp(random.uniform(0.3, 0.5))\n",
    "    repo = requests.get(url = url, headers = config)\n",
    "    repo.encoding = 'utf-8'\n",
    "    html = repo.text\n",
    "    # print(url)\n",
    "    return html\n",
    "\n",
    "def parse_html(html, url):\n",
    "    try:\n",
    "        soup = bs(html, \"lxml\") # 先解析网页\n",
    "        \n",
    "        # 新浪有个贼坑的专栏叫\"创事记\"，如果作者是他的话发行日期和文章截取得特判\n",
    "        author = re.findall('<meta name=\"mediaid\" content=\"(.*?)\"', html)[0] # 作者\n",
    "        if author == \"创事记\":\n",
    "            publish_time = soup.find(\"span\", id = \"pub_date\").text # 发行日期\n",
    "            publish_time = publish_time.strip()\n",
    "            date_str = publish_time\n",
    "            date_obj = datetime.strptime(date_str, \"%Y-%m-%d %H:%M:%S\")\n",
    "        else:\n",
    "            publish_time = soup.find(\"span\", class_ = \"date\").text # 发行日期\n",
    "            date_str = publish_time\n",
    "            date_obj = datetime.strptime(date_str, \"%Y年%m月%d日 %H:%M\")\n",
    "        \n",
    "        new_date_str = date_obj.strftime(\"%Y-%m-%d\")\n",
    "        publish_time = date_obj.strftime(\"%Y-%m-%d %H:%M\")\n",
    "        dir_name = re.findall('[0-9]\\/(.+?)\\.shtml', url)[0] # 给网站起个名字\n",
    "        dir_name = './websites_ten/' + new_date_str + dir_name\n",
    "        # print(dir_name)\n",
    "        try:\n",
    "            if not os.path.exists(dir_name):\n",
    "                os.mkdir(dir_name) # 一个网站一个文件夹\n",
    "                \n",
    "        except:\n",
    "            print(\"website already create\")\n",
    "        \n",
    "        dir_img = dir_name + '/images'\n",
    "        \n",
    "        try:\n",
    "            if not os.path.exists(dir_img):\n",
    "                os.mkdir(dir_img) # 一个网站一个文件夹\n",
    "                \n",
    "        except:\n",
    "            print(\"image site already create\")\n",
    "        \n",
    "        title = re.findall('<meta property=\"og:title\" content=\"(.*?)\"', html)[0] # 新闻标题\n",
    "        \n",
    "        description = re.findall('<meta name=\"description\" content=\"(.*?)\"', html) # 摘要\n",
    "        if len(description) > 0:\n",
    "            description = description[0]\n",
    "        keywords_source = soup.find(\"div\", class_ = \"keywords\") # 关键词-1\n",
    "        keywords = []\n",
    "        if keywords_source is not None:\n",
    "            keywords = keywords_source.find_all(\"a\") # 关键词-2\n",
    "            keywords = [item.text for item in keywords] # 关键词-3\n",
    "        # print(keywords)\n",
    "        passage_naked = \"\" # 整个文章\n",
    "        passage_origin = \"\"\n",
    "        passage = soup.select(\".article p\") # 直接元素选择器\n",
    "        if passage == []:\n",
    "            passage = soup.select(\"#artibody p\") # 直接元素选择器\n",
    "        for content in passage:\n",
    "            passage_naked += (content.text + \"\\n\")\n",
    "            passage_origin += content.text\n",
    "        # print(passage_naked)\n",
    "        dic_ = {\"url\": url, \"title\": title, \"author\": author, \"description\": description, \"publish_time\": publish_time, \"keywords\": keywords}\n",
    "        json_dic = json.dumps(dic_, indent = 2) # 将以上文件通过dict转化为json utf-8\n",
    "        # print(json_dic)\n",
    "        json_dir = dir_name + '/config.json' # json文件的地址\n",
    "        article_dir = dir_name + '/article.txt' # 文章内容地址\n",
    "        article_without = dir_name + '/article_origin.txt' # 文章内容地址\n",
    "        with open(json_dir, 'w') as f:\n",
    "            f.write(json_dic) # 写入config.json文件\n",
    "            f.close()\n",
    "        \n",
    "        with open(article_dir, 'w') as f:\n",
    "            f.write(passage_naked) # 写入article.txt文件\n",
    "            f.close()\n",
    "            \n",
    "        with open(article_without, 'w') as f:\n",
    "            f.write(passage_origin) # 写入article.txt文件\n",
    "            f.close()\n",
    "            \n",
    "        img_links = []\n",
    "        imgs = soup.find_all(\"div\", class_ = \"img_wrapper\")\n",
    "        for item in imgs:\n",
    "            img_link_origin = item.find(\"img\")\n",
    "            #print(img_link_origin)\n",
    "            try:\n",
    "                img_link_origin = img_link_origin[\"src\"]\n",
    "                if not(re.match(\"^https:\", img_link_origin) or re.match(\"^http:\", img_link_origin)):\n",
    "                    img_link_origin = \"https:\" + img_link_origin\n",
    "                if img_link_origin is not None:\n",
    "                    img_links.append(img_link_origin)\n",
    "            except:\n",
    "                pass\n",
    "        # print(img_links)\n",
    "        index = 0\n",
    "        img_dic = {\"empty\": 1} #图片对应的id和链接\n",
    "        \n",
    "        datax = {}\n",
    "        for one in img_links:\n",
    "            img_dic[\"empty\"] = 0;\n",
    "            index += 1\n",
    "            name_tmp = one.split('/')[-1]\n",
    "            datax[f\"{index}\"] = [name_tmp, one];\n",
    "            img_data = requests.get(url = one).content\n",
    "            with open(os.path.join(dir_img, name_tmp), 'wb') as f:\n",
    "                f.write(img_data)\n",
    "                f.close()\n",
    "        \n",
    "        img_dic['data'] = datax\n",
    "        img_json_dic = json.dumps(img_dic, indent = 2)\n",
    "        json_dir = dir_img + '/img_config.json' # json文件的地址\n",
    "        with open(json_dir, 'w') as f:\n",
    "            f.write(img_json_dic) # 写入img_config.json文件\n",
    "            f.close()\n",
    "    except Exception as ex:\n",
    "        print(url)\n",
    "\n",
    "    \n",
    "    \n",
    "def main():\n",
    "    # with multiprocessing.Pool(processes=8) as pool:\n",
    "    with open(r\"./news_links_lasty.txt\", \"r\", encoding = \"utf-8\") as f:\n",
    "        urlsx = f.readlines()\n",
    "    urlsx = [ url.strip() for url in urlsx ]\n",
    "    for url in urlsx:\n",
    "        parse_html(get_html(url), url)\n",
    "        \n",
    "        \n",
    "if __name__ == \"__main__\":  \n",
    "   main() "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "之后只要让所有报错的url去执行第二个就可以了~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#以下是专门为新浪的苹果新闻定制的小爬虫\n",
    "from PIL import Image\n",
    "config = {\"User-Agent\": \n",
    "            \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/116.0.0.0 Safari/537.36 Edg/116.0.1938.54\",\n",
    "        }\n",
    "with open(r\"./pa2.txt\", \"r\", encoding = \"utf-8\") as f:\n",
    "    urlsx = f.readlines()\n",
    "# print(urlsx)\n",
    "urlsx = [ url.strip() for url in urlsx ]\n",
    "# print(urlsx)\n",
    "for url in urlsx:\n",
    "    try:\n",
    "        slp(random.uniform(0.3, 0.5))\n",
    "        repo = requests.get(url = url, headers = config)\n",
    "        repo.encoding = 'utf-8'\n",
    "        html = repo.text\n",
    "        # print(html)\n",
    "        soup = bs(html, \"lxml\") # 先解析网页\n",
    "        \n",
    "        #关键词模块重做：根据观察，放在meta里面了\n",
    "        keywords = re.findall(r'<meta name=\"keywords\" content=\"(.*?)\"' , html)[0]\n",
    "        keywords = keywords.split(',')\n",
    "        # print(keywords)\n",
    "        \n",
    "        title = re.findall('<meta property=\"og:title\" content=\"(.*?)\"', html)[0] # 新闻标题\n",
    "        author = re.findall('<meta property=\"article:author\" content=\"(.*?)\"', html)[0] # 作者\n",
    "        # print(author)\n",
    "        \n",
    "        #发行日期模块重做\n",
    "        publish_time = re.findall(r'<span id=\"pub_date\">(.*?)</span>', html)[0] # 发行日期\n",
    "        date_obj = datetime.strptime(publish_time, \"%Y年%m月%d日%H:%M\")\n",
    "        new_date_str = date_obj.strftime(\"%Y-%m-%d\")\n",
    "        publish_time = date_obj.strftime(\"%Y-%m-%d %H:%M\")\n",
    "        # print(new_date_str)\n",
    "        dir_name = re.findall('[0-9]\\/(.+?)\\.shtml', url)[0] # 给网站起个名字\n",
    "        dir_name = './websites/' + new_date_str + dir_name\n",
    "        # print(dir_name)\n",
    "        try:\n",
    "            if not os.path.exists(dir_name):\n",
    "                os.mkdir(dir_name) # 一个网站一个文件夹\n",
    "                dir_img = dir_name + '/images'\n",
    "                os.mkdir(dir_img) # 文件夹中有一个图片夹\n",
    "        except:\n",
    "            print(\"website already create\")\n",
    "\n",
    "        description = re.findall('<meta name=\"description\" content=\"(.*?)\"', html) # 摘要\n",
    "        if len(description) > 0:\n",
    "            description = description[0]\n",
    "        # print(keywords)\n",
    "        passage_naked = \"\" # 整个文章\n",
    "        passage = soup.select(\"#artibody p\") # 直接元素选择器\n",
    "        for content in passage:\n",
    "            passage_naked += (content.text + \"\\n\")\n",
    "        # print(passage_naked)\n",
    "        dic_ = {\"url\": url, \"title\": title, \"author\": author, \"description\": description, \"publish_time\": publish_time, \"keywords\": keywords}\n",
    "        json_dic = json.dumps(dic_, indent = 2) # 将以上文件通过dict转化为json utf-8\n",
    "        # print(json_dic)\n",
    "        json_dir = dir_name + '/config.json' # json文件的地址\n",
    "        article_dir = dir_name + '/article.txt' # 文章内容地址\n",
    "        with open(json_dir, 'w') as f:\n",
    "            f.write(json_dic) # 写入config.json文件\n",
    "            f.close()\n",
    "        \n",
    "        with open(article_dir, 'w') as f:\n",
    "            f.write(passage_naked) # 写入article.txt文件\n",
    "            f.close()\n",
    "            \n",
    "        img_links = []\n",
    "        imgs = soup.find_all(\"div\", class_ = \"img_wrapper\")\n",
    "        for item in imgs:\n",
    "            img_link_origin = item.find(\"img\")[\"src\"]\n",
    "            if not re.match(\"^https:\", img_link_origin):\n",
    "                img_link_origin = \"https:\" + img_link_origin\n",
    "            img_links.append(img_link_origin) \n",
    "        # print(img_links)\n",
    "        index = 0\n",
    "        img_dic = {\"empty\": 1} #图片对应的id和链接\n",
    "        \n",
    "        for one in img_links:\n",
    "            img_dic[\"empty\"] = 0;\n",
    "            index += 1\n",
    "            name_tmp = one.split('/')[-1]\n",
    "            img_dic[f\"{index}\"] = name_tmp;\n",
    "            img_data = requests.get(url = one).content\n",
    "            with open(os.path.join(dir_img, name_tmp), 'wb') as f:\n",
    "                f.write(img_data)\n",
    "                f.close()\n",
    "                \n",
    "        img_json_dic = json.dumps(img_dic, indent = 2)\n",
    "        json_dir = dir_img + '/img_config.json' # json文件的地址\n",
    "        with open(json_dir, 'w') as f:\n",
    "            f.write(img_json_dic) # 写入img_config.json文件\n",
    "            f.close()\n",
    "    except Exception as ex:\n",
    "        print(url)\n",
    "        print(repo)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "至此，我们得到了全部8776篇新闻的概述（json保存）、图片（全部下载）、文本。\n",
    "### 网站部分\n",
    "* 主要结构：django的基本结构，以及一个专门用来导入新闻的脚本，下面是脚本："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import traceback\n",
    "import numpy as np\n",
    "from news.models import News, Category\n",
    "\n",
    "path = r'D:\\involuntary\\works\\missing-semester-of-python\\part1\\websites'\n",
    "files = os.listdir(path)\n",
    "\n",
    "# News.objects.all()\n",
    "\n",
    "for file in files:\n",
    "    try:\n",
    "        all_path = path + '\\\\' + file\n",
    "        config_path = all_path + r'\\config.json'\n",
    "        img_path = all_path + r'\\images\\img_config.json'\n",
    "        article_path = all_path + r'\\article.txt'\n",
    "        content = {}\n",
    "        contenTxt = ''\n",
    "        \n",
    "        # 导入所有文件\n",
    "        with open(config_path, 'r') as f:\n",
    "            json_config = f.read() \n",
    "        xconfig = json.loads(json_config)\n",
    "        with open(article_path, 'r', encoding='utf-8') as g:\n",
    "            pas = g.readlines()\n",
    "            index = 0\n",
    "            for item in pas:\n",
    "                content[f'{index}'] = item.rstrip()\n",
    "                contenTxt += item.rstrip()\n",
    "                index += 1\n",
    "        with open(img_path, 'r') as h:\n",
    "            img_config = h.read()\n",
    "            img_bababa = json.loads(img_config)\n",
    "        short = content['0'][:50] if content['0'] else \"无预览\"\n",
    "        category = \"\"\n",
    "        now_time = datetime.now()\n",
    "        pub_time = datetime.strptime(xconfig['publish_time'], '%Y-%m-%d %H:%M')\n",
    "        if pub_time + timedelta(days=60) > now_time:\n",
    "            category = \"两个月以内\"\n",
    "        elif pub_time + timedelta(days=90) > now_time:\n",
    "            category = \"三个月以内\"\n",
    "        elif pub_time + timedelta(days=120) > now_time:\n",
    "            category = \"四个月以内\"\n",
    "        else:\n",
    "            category = \"其他\"\n",
    "        xx = f'{file}\\\\images\\\\'\n",
    "        whiw = [(xx + f'{img_bababa[\"data\"][key][0]}') for key in img_bababa['data'].keys()]\n",
    "        new_news = News(config=json_config,\n",
    "                        pub_time=pub_time,\n",
    "                        read_num=np.random.randint(low=10, high=100000),\n",
    "                        author=xconfig['author'],\n",
    "                        origin=xconfig['url'],\n",
    "                        title=xconfig['title'],\n",
    "                        description=xconfig['description'],\n",
    "                        category_title=category,\n",
    "                        content=content,\n",
    "                        short_arg=short + \"......\",\n",
    "                        img_config=img_config,\n",
    "                        comment_num=0,\n",
    "                        contenTxt=contenTxt,\n",
    "                        category=Category.objects.get(title=category),\n",
    "                        image_dir=whiw,\n",
    "                        )\n",
    "        new_news.save()\n",
    "    except:\n",
    "        print(traceback.print_exc())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* models部分：我使用了Category、News和Comments三个模型，分别进行Cat -> N与N -> Com的外键关联，使得一个分类可以直接管理多条新闻、一条新闻可以直接管理多个评论，同时利用数据库的默认排序，将默认值改为了按发布时间先后，这样子就在数据输入阶段他就会自动按照时间排序了："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from django.db import models\n",
    "\n",
    "\n",
    "# Create your models here.\n",
    "\n",
    "class Category(models.Model):\n",
    "    \"\"\"\n",
    "    a certain category\n",
    "    \"\"\"\n",
    "    title = models.CharField(max_length=200)\n",
    "    app_label = 'news'\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.title\n",
    "\n",
    "    class Meta:\n",
    "        app_label = 'news'\n",
    "\n",
    "\n",
    "class News(models.Model):\n",
    "    \"\"\"\n",
    "    a certain news\n",
    "    \"\"\"\n",
    "    config = models.JSONField()  # 真实的json\n",
    "    # 有一些变量其实是可以不用的(毕竟我都有json了) 我还是给他弄上了\n",
    "    # 毕竟这样的调用要正常不少\n",
    "    pub_time = models.DateTimeField()  # 发布时间\n",
    "    short_arg = models.TextField()\n",
    "    category_title = models.CharField(max_length=200, null=True)\n",
    "    contenTxt = models.TextField(null=True)\n",
    "    read_num = models.IntegerField()  # 热 度\n",
    "    author = models.CharField(max_length=200)  # 作者\n",
    "    origin = models.URLField()  # 原新闻链接\n",
    "    title = models.CharField(max_length=200)  # 标题\n",
    "    description = models.TextField()  # 摘要\n",
    "    content = models.JSONField()  # 内容\n",
    "    img_config = models.JSONField()\n",
    "    comment_num = models.IntegerField()\n",
    "    image_dir = models.JSONField(null=True)\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.title\n",
    "\n",
    "    # 关联外键\n",
    "    category = models.ForeignKey(\n",
    "        'Category',\n",
    "        on_delete=models.SET_NULL,\n",
    "        null=True,\n",
    "    )\n",
    "\n",
    "    class Meta:\n",
    "        # News.objects提出数据时按照列表指定的字段排序\n",
    "        ordering = ['-pub_time']\n",
    "\n",
    "\n",
    "class Comment(models.Model):\n",
    "    \"\"\"\n",
    "    a certain comment\n",
    "    其实没什么大的要求，有内容有时间就很足够了\n",
    "    \"\"\"\n",
    "    # author = models.CharField(max_length=200)  # 作者\n",
    "    content = models.TextField()\n",
    "    pub_time = models.DateTimeField(auto_now_add=True)  # 这样就该是多少就是多少了\n",
    "\n",
    "    # 关联外键\n",
    "    news = models.ForeignKey(\n",
    "        'News',\n",
    "        on_delete=models.CASCADE,  # 跟随删除\n",
    "        null=False,\n",
    "    )\n",
    "\n",
    "    class Meta:  # 画瓢\n",
    "        ordering = ['-pub_time']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 所有的分页部分我都使用了django自带的分页器进行分页，以下是一个示例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def news_list(request):\n",
    "    # news/list/?page=114514   params = {\"page\": 114514} 模拟新浪的处理方式\n",
    "    all_news = News.objects.all()\n",
    "    paginator = Paginator(all_news, 10)\n",
    "    page_number = int(request.GET.get('page', 1))  # 自动填充一个param，如果有page那就是page，没有就默认1\n",
    "    if page_number > paginator.num_pages:\n",
    "        page_number = paginator.num_pages\n",
    "    elif page_number < 1:\n",
    "        page_number = 1\n",
    "    page = paginator.page(page_number)\n",
    "    if page_number < 6:  # 没有别的工具，只能手动操作力\n",
    "        if paginator.num_pages <= 10:\n",
    "            page_range = range(1, paginator.num_pages + 1)\n",
    "        else:\n",
    "            page_range = range(1, 11)\n",
    "    elif (page_number >= 6) and (page_number <= paginator.num_pages - 5):\n",
    "        page_range = range(page_number - 5, page_number + 5)\n",
    "    else:\n",
    "        page_range = range(paginator.num_pages - 9, paginator.num_pages + 1)\n",
    "\n",
    "    return render(request, 'list.html', {'page': page,\n",
    "                                         'range': page_range,\n",
    "                                         'current_num': page.number,\n",
    "                                         'news': page.object_list,\n",
    "                                         'num_pages': paginator.num_pages,\n",
    "                                         'has_previous': page.has_previous(),\n",
    "                                         'has_next': page.has_next()})\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 在搜索引擎部分，为了支持能够分词搜索（即我如果搜“显卡的电脑”，能够出现所有带“显卡”和“电脑”全部二者的新闻的搜索结果），我使用了jieba库对搜索文本进行分词并且结合内置数据库进行搜索，搜索时间基本能控制在0.4s左右："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if query:\n",
    "    x_blink = Q(title__icontains=query) | Q(contenTxt__icontains=query)\n",
    "    found_news_blink = None\n",
    "    sera_list = [word for word in jieba.cut_for_search(query) if word not in stop_words]\n",
    "    for item in sera_list:\n",
    "        if found_news_blink is None:\n",
    "            found_news_blink = (Q(title__icontains=item) | Q(contenTxt__icontains=item))\n",
    "        else:\n",
    "            found_news_blink &= (Q(title__icontains=item) | Q(contenTxt__icontains=item))\n",
    "    found_news = all.filter(x_blink | found_news_blink)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 而所有的刷新功能和翻页功能我都使用了JavaScript来进行：\n",
    "```javascript\n",
    "function shift(page) {\n",
    "        let url = \"/news/search/\";\n",
    "        url += \"?page=\" + page + \"&query=\" + \"{{query}}\" + \"&sort=\" + {{sorting}};\n",
    "        {% for item in month_list %}\n",
    "        url += (\"&month=\" + {{item}})\n",
    "        {% endfor %}\n",
    "        window.location.href = url;\n",
    "    }\n",
    "\n",
    "function refresh() {\n",
    "        window.location.reload(\"news_random\");\n",
    "    }\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据分析\n",
    "使用了jieba分词、pandas存储分词后的数据（键值对形式）、wordcloud绘制词云和matplotlib绘制柱状图，详见data_ana.ipynb数据分析报告\n",
    "### 所有项目共耗时\n",
    "* 爬虫及其优化：2天，共计12小时左右\n",
    "* web框架：6天，共计25小时左右（因为生病，每天的工作时间都减少）\n",
    "* 数据分析：1天半，共计5小时左右"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mach",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
